command line tools for interacting with kubernetes cluster
1. kubectl is a controller programe for kubernetes.
2. minikube is a cut down version of Kubernetes, and we use this for our local development.

Install docker:
sudo -i
sudo yum install docker



kubectl Installation:
curl -LO https://storage.googleapis.com/kubernetes-release/release/`curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt`/bin/linux/amd64/kubectl

chmod +x kubectl 

sudo mv ./kubectl /usr/bin/kubectl


minikube Installation:
curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64

chmod +x minikube 

sudo mv ./minikube /usr/bin/

minikube start --vm-driver=none

Stopping Minikube
/usr/local/bin/minikube stop




Misc Commands:
minikube docker-env
minikube ip


Pod:
A pod is a group of one or more containers(such as Docker containers, we can run k8s in non docker environments also) with shared storage/network and a specification for how to run the containers.

*** POD is just a wrapper over the container.

*** Every Micro service should run in a separate pod
Most of the times only ONE container will run in a pod
but for a rare cases we can have a helper container along with the main container in the pod.


Pods are running in a kubernetes cluster.
*** Pods are NOT VISIBLE outside of k8s cluster, because pods are short lived ones or new pods will be created dynamically so it is not exposed to the outside world.

*** kubectl is a command line tool where it will do Rest API post call Rest API service “kubernetes” (service/kubernetes) in a k8s cluster.

kubectl get all
will get all the services, pods etc running in a k8s cluster.

Running a pod:
kubectl apply -f fileName.yaml
here f indicates file 
e.g.: kubectl apply -f first-pod.yaml

Describing information about pod
kubectl describe pod <podName>
eg: kubectl describe pod webapp


Executing commands inside pods through command line:
kubectl exec podName <commandName>
eg: kubectl exec webapp ls

Running pod interactively:
kubectl -it exec podName sh
e.g.: kubectl -it exec web sh


Service:

Pods are not visible outside of k8s cluster(because pods are short lived and can be dynamically created at runtime), for this we have Service in k8s which are long lived object.

How K8s will map the Service to Pod
Pod have labels (can have multiple labels) - a key value pair
Service have Selector - a key value pair
If the selector and label matches then those will be logically mapped by k8s

Pod label
app: webapp

Service Selector
app: webapp

Here we can multiple labels and multiple selectors, if we have multiple selectors then all  of those should be there in the labels to have a logical connection between service and pods.


Pod label
app: webapp
release: “0”
key1: value1

Service Selector
app: webapp
release: “0”


This Service can be of 2 types:
1. ClusterIP
2. NodePort

ClusterIP is an internal service, means this service will not be accessed from outside of k8s cluster.
This is the most commonly used service type as most of the micro services are deployed in private network and should not be accessed from outside of the network DIRECTLY.


This is used when we are running in a cluster and in the cloud
LoadBalancer - this is an external service 

This is used when we are running in a cluster and the service is not a load balancer
NodePort - this is an external service

*** NodePort range should be >30000, this is a restriction in k8s.


commands:
List all pods, services etc
kubectl get all

List only pods
kubectl get pods

List only services
kubectl get services

List pods with labels
kubectl get pods --show-labels

List pods with labels and with a filter
kubectl get pods --show-labels -l release=0

Deleting Pods:
kubectl delete pods <podname>
eg: kubectl delete pods webapp

Deleting ALL pods:
kubectl delete pods --all


Shortcuts for different resource types
pods - po
service - svc
replicaset - rs



ReplicaSet:
If we manually run the pod then pod lifecycle is NOT managed by K8s, means if the pod crashes for some reason then another pod for that application will not be spin up AUTOMATICALLY.

for this to happen we need to run pods using ReplicaSet.


*** In ReplicaSet we need a Selector so that a group of pods can be identified under a ReplicaSet
*** In ReplicaSet Pod name is not needed as it is auto generated by ReplicaSet.


Deployment:
It is same as as Replica set but with an additional feature of rolling updates with zero down time.
Here 
Deployment manages Replica sets
ReplicaSet manages Pods

Whenever a new version is deployed using Deployment option what will happen is it will create a new Replica set for the new version and Destroys the pods in the old Replica set (old version) ONLY WHEN THE service with New Version is Ready to serve/responding to  the requests.

*** Here Kubernetes will destory pods in the Replica set but not replica set because if we want to rollout to the old version then kubernetes can reuse that replica set and create pods in that replica set.

This means for some reason if the new version is not able to start then our application will STILL RUN WITH OLD VERSION. This is how zero downtime is acheived.


*** With Deployment we can do a roll back to older version if some of the feature is not working as expected.

"Rollout status" command wil give summary of how roll out of new version is happening
kubectl rollout status deployment <deployment-name>
kubectl rollout status deployment webapp

Rollout history:
kubectl rollout history deployment webapp

By Default Kubernetes will remember 10 revisions of rollouts. 

If we want to rollout to the previous version
Below command will rollout to previous version
kubectl rollout undo deployment webapp  

If we want to rollout to a specific version
kubectl rollout undo deployment webapp --to-revision=2


Networking and Service Discovery:
How Pods will interact with other Pods in k8s cluster
this will happen via DSN (kube-dns) that k8s run automatically behind the scenes.
This DNS will have a registry of pod name and internal ip of that pod which will assigned dynamically by k8s.
eg: webapp 10.9.3.12
    database 10.9.3.14


Namespaces:
we can segragate resources (pods/services/rs/deployments) into a group called namespaces.
If you dont provide a namespace then "default" namespace will be taken.

kubectl get namespaces 
or
kubectl get ns

In the cluster we have below namespaces
default
kube-public
kube-system

we can have a namespaces created for all third party resources.

Executing command for a specific namespace:
kubectl get all --namespace kube-system
kubectl get all -n kube-system

DNS:
Pods in k8s will communicate via DNS running in k8s cluster.

eg:
NAME               TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                  AGE
service/kube-dns   ClusterIP   10.96.0.10   <none>        53/UDP,53/TCP,9153/TCP   6d13h


cat /etc/resolv.conf
this file is used to find the dns server ip

in this command if we get search option it means if the hostname is not found then it should follow this order
hostname
hostname.search1
hostname.search2
.
.
.



eg:
search default.svc.cluster.local svc.cluster.local cluster.local 
and if hostname is database then the search order will be 

database
database.default.svc.cluster.local
database.svc.cluster.local
database.cluster.local

*** Here if you observe default is the namespace in the k8s pod
so if your service is in the different namespace then we should search with
database.<namespace>


DNS inside the pod
[root@ip-172-31-16-183 ~]# kubectl exec -it webapp-858957ccc9-9lnmx sh
/ # cat /etc/resolv.conf 
nameserver 10.96.0.10
search default.svc.cluster.local svc.cluster.local cluster.local ap-south-1.compute.internal
options ndots:5


Installing mysql in Apline Image
apk update
apk add mysql-client
mysql -h database -uroot -ppassword



*** nslookup is used to get the ip for the hostname in the network.
eg: nslookup database
    nslookup google.com


 / # nslookup google.com
nslookup: can't resolve '(null)': Name does not resolve

Name:      google.com
Address 1: 216.58.199.174 bom05s08-in-f174.1e100.net
Address 2: 2404:6800:4009:80b::200e bom07s16-in-x0e.1e100.net
/ # nslookup database
nslookup: can't resolve '(null)': Name does not resolve

Name:      database
Address 1: 10.96.194.78 database.default.svc.cluster.local   


If we want to delete deployments, replica set and pods then we can use below commands as per requirement.

Deletes service fleetman-webapp
kubectl delete service fleetman-webapp

This will apply mysql-network-test.yaml yaml files and deletes services, deployment, replica set, pods if exist
kubectl delete -f mysql-network-test.yaml

Deletes deployment
kubectl delete deployment webapp

This will apply all yaml files and deletes services, deployment, replica set, pods if exist
kubectl delete -f .


How to get logs of pod
kubectl logs -f pod/position-simulator-7d555dc46-qkf5w

Getting Persistent Volume Claim
kubectl get pvc

Getting Persistent Volumes
kubectl get pv

K8s on AWS:

Install kops on the node machine

curl -LO https://github.com/kubernetes/kops/releases/download/$(curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d '"' -f 4)/kops-linux-amd64
chmod +x kops-linux-amd64
sudo mv kops-linux-amd64 /usr/local/bin/kops


Create kops user with following Access
AmazonEC2FullAccess
AmazonRoute53FullAccess
AmazonS3FullAccess
IAMFullAccess
AmazonVPCFullAccess

or via command line tool
aws iam create-group --group-name kops

aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonEC2FullAccess --group-name kops
aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonRoute53FullAccess --group-name kops
aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess --group-name kops
aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/IAMFullAccess --group-name kops
aws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonVPCFullAccess --group-name kops

aws iam create-user --user-name kops

aws iam add-user-to-group --user-name kops --group-name kops

aws iam create-access-key --user-name kops


Configure the AWS Command Line to use this new user
# configure the aws client to use your new IAM user
aws configure           # Use your new access and secret key here
aws iam list-users      # you should see a list of all your IAM users here

# Because "aws configure" doesn't export these vars for kops to use, we export them now
export AWS_ACCESS_KEY_ID=$(aws configure get aws_access_key_id)
export AWS_SECRET_ACCESS_KEY=$(aws configure get aws_secret_access_key)



We need to setup S3 Bucket:
This is where kops will store working data.


Create Cluster:
export NAME=fleetman.k8s.local
export KOPS_STATE_STORE=s3://leela-kops-state-storage

*** Here .k8s.local is needed as we are using gossip based cluster which doesn't need DNS setup.


To List down Availability Zones in a region
aws ec2 describe-availability-zones --region ap-south-1


Creating K8S Cluster
kops create cluster --zones ap-south-1a,ap-south-1b,ap-south-1c ${NAME}

Here NAME is cluster name which is set in environment variable.
This will not start the cluster with master and slave nodes instead it will create configuration files that is required for the cluster.

ssh-keygen -b 2048 -t rsa -f ~/.ssh/id_rsa
kops create secret --name fleetman.k8s.local sshpublickey admin -i ~/.ssh/id_rsa.pub


How to edit Cluster Configuration
kops edit cluster ${NAME}

How to edit Instance Group Configuration(where our instance details in the cluster will be there like instance type, min and max instances)
kops edit ig nodes --name ${NAME}

How to get instance group details
kops get ig --name ${NAME}

How to edit master node instance details
kops edit ig ${MASTER_NODE_NAME} --name ${NAME}
kops edit ig master-ap-south-1a --name ${NAME}


Starting Cluster
kops update cluster --name ${NAME} --yes

This will start the cluster means
it will start the master and the worker nodes that are defined in the configuration files
like number of nodes, type of nodes(medium or large etc)

*** Since Master and Worker nodes have min and max nodes, so Autoscaling groups are created for both Master and Worker nodes i.e., 2 ASG are created
so that in case any node goes down then ASG will take care of spining a new node.

*** When Cluster started there will be one load balancer created for Master Node(s), this is because all the commands issued via kubectl will be executed against Master Node and some times Master Node can crash and new Node will spin up which will have different ip address. So to have a stable DNS name this master node(s) will backed by a Elastic Load Balancer so that in case of restart DNS of ELB will not change.

kops validate cluster

Note: If any worker node is crashed then k8s will detect that and will restart all the pods that are there in that worker node on another nodes in the cluster.
Nodes and pods are different.


How to know which pods are running on which Nodes?
kubectl get pods -o wide


Deleting K8S Cluster:
kops delete cluster --name ${NAME} --yes


DeamonSet - This will run the pod in every Node in Kubernetes Cluster
StatefulSet - This will not generate random numbers instead it will suffix every instance with -0, -1, -2
so that this can be used instances can be used in the cluster.

*** When we run Elastic search, Fluentd and Kibana it will start all resources like pods, services etc in kube-system namespace


What if Master Node crashes?
If Master Node is down then external/outside traffic to K8S cluster will be down and there is no catastrophe effect on the cluster and the cluster will run fine. This is because Master Node is responsible for administration tasks, kubectl tasks and rescheduling K8S resources(like pods, services etc) on other nodes.

When Master is Down and one of the Node in the cluster crashes then pods in this node will not be restarted until Master is healthy, This is the only impact on the cluster. 


Memory and CPU Requests, Limits in K8S:
This specification should be at container level.

resources:
  requests:
    memory: 300Mi
    cpu: 100m
  limits:
    memory: 500Mi
    cpu: 200m

In the specification we have difference between 'M' and 'Mi'
M - 1000Kb
Mi - 1024Kb

K - 1000 bytes 
ki - 1024 bytes

100m - 0.1 cpu core
1000m - 1 cpu core

Here requests section will give a hint to the scheduler running in the master node that a POD NEEDS TO BE CREATED ONLY WHEN THERE IS ATLEAST SPECIFIED AMOUNT OF MEMORY AND CPU.

This will ensure that pod will get the minimum amount of RAM and CPU, and prevent node being overloaded.
If we don't have these requests section then scheduler can launch more number of pods on the same Node.

requests section will only mention about the minimum amount of RAM and CPU that is needed, but if application needs more RAM or CPU it can take from the node. This can lead to Resource Starvation for other pods that is running on that Node.

To avoid this starvation we have to set the limits for a pod that it can go. This is mentioned in the limits section.

*** If the Pod is taking more RAM memory that is mentioned in the limits section THEN THE CONTAINER IN THE POD WILL BE KILLED AND RESTARTED BUT POD WILL NOT BE KILLED.

*** If the Pod is taking more CPU that is mentioned in the limits section then CPU will be clamped or throttled.


Metrics on Kubernetes Cluster:
In K8S Cluster we can see memory(RAM) and cpu utilization on pods or nodes
By default this metrics option is disabled in the addon list and we need to enable.

To list all the addons present in K8S cluster
minikube addons list

Enabling metrics server
minikube addons enable metrics-server

** This will run a metrics-server pod and service in kube-system namespace

Metrics on Pod level
kubectl top pod

Metrics on node level
kubectl top node

By Profiling or by using this top condition we can understand how much RAM or CPU is needed by a pod. 


We can see these metrics on a dashboard that kubernetes provides out of the box, In this dashboard we can do all operations that we will do using kubectl command.
For this we need to enable dashboard addon

minikube addons enable dashboard

This will run a dashboard pod and service in kube-system namespace.

Launching Dashboard:
minikube dashboard


Horizontal Pod AutoScaling (HPA)
When there huge load on a specific interval then we may need to scale pods horizontally (means replicating the pod) so that the load will be shared across pods.
*** Every Pod is not eligible for Auto scaling 
eg: databases, Message Queues
If we need to run multiple instances of database and MQ then we need to use there own clustering mechanism to balance the load.

*** Stateless Services can be auto scaled 
eg: API Gateway

HPA through command line:
kubectl autoscale deployment <service-name> --cpu-percent <max-limit> --min <min-pods> --max <max-pods>
eg: kubectl autoscale deployment api-gateway --cpu-percent 400 --min 1 --max 4

*** Setting Max Number of Pods is important otherwise lots of pods will be launched during heavy loads which may bring down entire K8S cluster

Getting HPA in K8S cluster:
kubectl get hpa

Describing HPA:
kubectl describe hpa <service-name>
eg: kubectl describe hpa api-gateway


Auto Generate YAML file from HPA command:
*** If we delete the cluster then again we need to issue the HPA command so that it will be set, to avoid this we can maintain this HPA information in a separate YAML file in source control system.

We can automatically Generate this yaml by using below command
kubectl get hpa <service-name> -o yaml

eg: kubectl get hpa api-gateway -o yaml


Deleting HPA:
kubectl delete hpa <service-name>
eg: kubectl delete hpa api-gateway


Liveness and Readiness:
When Horizontal pod scaling happened then new pods will come and then load balancer where all kubectl commands will go will balance this pods.
The problem here is pods may NOT be started immediately(depends on the technology/framework) and the load balancer doesn't about this one as the pod status is in Running state.
To solve this problem we can provide a Readiness Probe(i.e., health check) so that the load balancer will do the health check first and ensure that pod is really functioning or not. If health check not passed then it will not send traffic to that pod.

eg: 
readinessProbe:
  httpGet:
    path: /
    port: 8080

Testing this Readiness
while true; do curl 13.127.115.93:30080/api; done;   


Quality Of Service(QOS):
K8S Scheduler will schedule pods on the node based on the requests and limits, and if no requests and limits is specified it will still schedule the pod on that node if it has memory.
K8S will give below labels for different use cases
1. QOS: Guaranteed
2. QOS: Burstable
3. QOS: BestEffort

If the pod defines both requests and limits(it is for both cpu and RAM) then that pod will be labeled as QOS:Guaranteed.
*** here request and limits should be same then only the pod will be Guaranteed otherwise it is Burstable.
eg: 
request: 500Mb
limit: 500Mb

If the pod defines only requests then that pod will be labeled as QOS:Burstable
eg:
request: 300Mb

If the pod doesn't define any requets and limits then that pod will be labeled as QOS:BestEffor

These labels are used to evicte a pod when node starts out of resources.
If the node ran out of resources then it will evict BestEffort Pod, still if it doesn't have enough resources then it will evict Burstable Pod, still if it doesn't have enough resources then it will evict Guaranteed Pod

Eviction Precedence:
1. BestEffort
2. Burstable
3. Guarantted

*** When a Pod is Evicted then K8S will schedule that pod on a different node in the cluster if it has enough resources.
***  If all the nodes in K8S doesn't have enough resources, then K8S cannot schedule the pod in this case if Cluster Auto Scaling is Enabled then it will dynamically create a NODE in the cluster. 


Pod Priorities:
We can give a pod priroity, and this priroity is used for scheduling a pod on the node.
If there is no enough resources then pod with less priority will be evicted and pod with high priroity will be scheduled on that node. So for scheduling K8S will consider Priority as the factor when the node is not having enough resources.

But During Eviction K8S will consider both Priority and QOS to find which pod that needs to be evicted.

In a Robust Micro services architecture all pods should have same prirority this is because at any point of time pod can be evicted and restarted on a different node in the cluster.

But if you have a situation where a pod should not evicted and scheduled immediately then we can use pod priorities.

RBAC(Role Based Access Control):
*** If we want to access K8S Cluster then we need a certificate that is signed by Kubernetes Certificate Authority.
when we generate K8S Cluster using Kops in AWS that user will automatically assigned a certificate signed by K8S CA so that the user who created cluster will be able to access Cluster Automatically.

If a new user wants to access K8S cluster then we need generate a certificate that is signed by Kubernetes Certificate Authority.


Before Generating certificate for a user we will define the Role that a user can do in the cluster
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: new-joiner
  namespace: default
rules:
  - apiGroups: [""] # empty represents access to resources that are having apiVersion as v1 this is also referred as core group
    resources: ["*"] # * means all K8S resources like pods, services, deployments etc
    verbs: ["get", "list", "watch"] # actions that user can perform on the resources.

---
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: put-specific-user-or-users-into-new-joiner-role
  namespace: default
subjects:
  - kind: User
    name: francis-linux-login-name
roleRef:
  kind: Role
  name: new-joiner
  apiGroup: rbac.authorization.k8s.io

How to list roles:
kubectl get roles

kubectl get role <role-name>
eg: kubectl get role new-joiner

How to describe roles:
kubectl describe role <role-name>
eg: kubectl describe role new-joiner

Create playground namespace to the new joiners:
kubectl create ns playground

Create new user:
sudo useradd francis-linux-login-name
sudo passwd francis-linux-login-name


Viewing Kubernetes configuration:
kubectl config view

Now switch to new user
su - francis-linux-login-name

*** when we switch to this new user there will be empty kubernetes configuration, so now we need to point to our running K8S cluster so that we can execute kubectl cluster on that cluster.

kubectl config set-cluster fleetman.k8s.local --server=https://api.fleetman-k8s-local-tkmafs-347944231.eu-west-2.elb.amazonaws.com

https://api.fleetman-k8s-local-tkmafs-347944231.eu-west-2.elb.amazonaws.com -> this represents load balancer to the master node of K8S cluster.

kubectl config set-context mycontext --user francis-linux-login-name --cluster fleetman.k8s.local
we can have multiple context, so we need to setup current context for that user.

kubectl config use-context mycontext

Now we pointed to the K8S Cluster, but if we try to access K8S Cluster we will get a "certificate signed by unknown authority" error
kubectl get pods

For this we need to issue a certificte that is signed by K8S CA.
For this we need to generate Certificate Signing request which can be sent to K8S Admin to sign with his private key and generate a certificate that can be used to access K8S Cluster.

To Generate Certificate Signing Request(CSR) we need to generate private key
openssl genrsa -out private-key-francis.key 2048

CSR Generation
openssl req -new -key private-key-francis.key -out req.csr -subj "/CN=francis-linux-login-name/O=francis-linux-login-name"

To certify francis user we need to get the K8S Public and private keys which will be stored in AWS S3 as part of cluster setup

Private Key:
aws s3 ls s3://leela-kops-state-storage/fleetman.k8s.local/pki/private/ca/
aws s3 cp s3://leela-kops-state-storage/fleetman.k8s.local/pki/private/ca/{123}.key kubernetes.key
chmod 400 kubernetes.key
chmod 400 private-key-francis.key

Public Key:
aws s3 ls s3://leela-kops-state-storage/fleetman.k8s.local/pki/issued/ca/
aws s3 cp s3://leela-kops-state-storage/fleetman.k8s.local/pki/issued/ca/{123}.crt kubernetes.crt

Now Certify CSR file using kubernetes public and private key to generate francis crt file
openssl x509 -req -in req.csr -out francis.crt -CA kubernetes.crt -CAkey kubernetes.key -CAcreateserial -days 365


Now move or place francis private and public keys and Kubernetes public key in francis directory
sudo mkdir /home/francis-linux-login-name/.certs
sudo mv private-key-francis.key /home/francis-linux-login-name/.certs
sudo mv francis.crt /home/francis-linux-login-name/.certs
sudo mv kubernetes.crt /home/francis-linux-login-name/.certs

since we(ec2-user) moved the files the files user and group will be ec2-user we need to change as francis user
sudo chown -R francis-linux-login-name:francis-linux-login-name /home/francis-linux-login-name/.certs

*** Still we can't access Kubernetes cluster because we just generated files and moved to a directory.
we need to point these files in kubernetes configuration.
kubectl config set-credentials francis-linux-login-name --client-certificate=francis.crt --client-key=private-key-francis.key

we need to set the CA who signed our user also, so that if we want to use our company CA we can change here
kubectl config set-cluster fleetman.k8s.local --certificate-authority=kubernetes.crt

ConfigMap:
sometimes we use environment variables when we run our containers, and these env variables are used across multiple containers 
or
we need externalize properties from the yaml file which is a common requirement then we can use Config Maps, so that if we change config maps then there is no change in the yaml file.
** but still we need to bounce the resource(pod) i.e., kill and restart  to reflect the changes.

*** As of now there is NO Automic Reload of Config Maps by the K8S Pods, so there are 2 options
1. is to kill all the pods which are referencing the config map so that replica set will restart automatically
2. to create another version of config map(v2) and change the yaml file which are referencing the old config map with new version and apply the changes.

How to list configmaps
kubectl get cm

kubectl describe cm <name-of-config-map>

Sample ConfigMap:
apiVersion: v1
kind: ConfigMap
metadata:
  name: global-database-config
  namespace: default
data:
  database.url: "https://mydatabaseserver.somewhere.com:3306"
  database.password: "P@ssw0rd1"

How to Reference Config Map Properties in yaml files:
There are 2 ways

Approach1:
apiVersion: apps/v1
kind: Deployment
metadata:
  name: position-simulator
spec:
  selector:
    matchLabels:
      app: position-simulator
  replicas: 1
  template: 
    metadata:
      labels:
        app: position-simulator
    spec:
      containers:
      - name: position-simulator
        image: richardchesterwood/k8s-fleetman-position-simulator:release2
        env:
          - name: SPRING_PROFILES_ACTIVE
            value: production-microservice
          - name: DATABASE_URL
            valueFrom:
              configMapRef:
                name: global-database-config
                key: database.url
          - name: DATABASE_PASSWORD
            valueFrom:
              configMapRef:
                name: global-database-config
                key: database.password


In this approach we need to write 4 lines to reference one property from config map and if we have lot of properties then our yaml file will be bloated, so we use approach2 where all the properties will be referenced.

global-database-config-v2:
==========================
apiVersion: v1
kind: ConfigMap
metadata:
  name: global-database-config-v2
  namespace: default
data:
  DATABASE_URL: "https://mydatabaseserver.somewhere.com:3306"
  DATABASE_PASSWORD: "P@ssw0rd1"


Approach2 (using envFrom):
apiVersion: apps/v1
kind: Deployment
metadata:
  name: position-simulator
spec:
  selector:
    matchLabels:
      app: position-simulator
  replicas: 1
  template: 
    metadata:
      labels:
        app: position-simulator
    spec:
      containers:
      - name: position-simulator
        image: richardchesterwood/k8s-fleetman-position-simulator:release2
        env:
          - name: SPRING_PROFILES_ACTIVE
            value: production-microservice
        envFrom:
          - configMapRef:
              name: global-database-config-v2

Here all environment variables will be referenced from global-database-config-v2 configmap
** In this approach all the properties should follow environment variable naming standards.


Mounting Config File as Volume:
apiVersion: apps/v1
kind: Deployment
metadata:
  name: position-simulator
spec:
  selector:
    matchLabels:
      app: position-simulator
  replicas: 1
  template: 
    metadata:
      labels:
        app: position-simulator
    spec:
      containers:
      - name: position-simulator
        image: richardchesterwood/k8s-fleetman-position-simulator:release2
        env:
          - name: SPRING_PROFILES_ACTIVE
            value: production-microservice
        volumeMounts:
          - name: database-config-volume
            # This directory will be created in the container
            value: /etc/any/directory/config

      volumes:
        - name: database-config-volume
          configMap:
            name: global-database-config-v3

global-database-config-v3
=========================
apiVersion: v1
kind: ConfigMap
metadata:
  name: global-database-config-v3
  namespace: default
data:
  DATABASE_URL: "https://mydatabaseserver.somewhere.com:3306"
  DATABASE_PASSWORD: "P@ssw0rd1"

*** When we mount above config-v3 then kubernetes will create a 2 files as below
1. DATABASE_URL
2. DATABASE_PASSWORD
these files will have value as the content


Generally we want a single property file with content as all properties for this we need to define as below
apiVersion: v1
kind: ConfigMap
metadata:
  name: global-database-config-v3
  namespace: default
data:
  # Here | will allow us to specify value in multiple lines.
  database.properties: |
    database.url=https://mydatabaseserver.somewhere.com:3306
    database.password=P@ssw0rd1

Here database.properties file will be created and the file content will be the value (i.e., all the properties).
*** Here using | symbol we can write a value in multiple lines.


Generally we will maintain config maps as immutable objects
*** When there change in config map we will create a new version and edit the yaml files to refer new version of config map.
*** When a config map is not found the container will be restarted until config map is found
  Warning  FailedMount  26s (x7 over 58s)  kubelet, ip-172-31-31-188.ap-south-1.compute.internal  MountVolume.SetUp failed for volume "database-config-volume" : configmap "global-database-config-v4" not found


Secrets:
Secrets are also a key value pair just like config map but Secrets provide a first layer of defence that when you describe a secret you will not get the value stored in the keys.

kubectl get secrets
kubectl get secret <secret-name>

kubectl describe secret <secret-name>

But we can get full secret files by using below command
kubectl get secret <secret-name> -o yaml
For this we can write a rule so that normal users cannot describe a secret object.

*** values in a secret should be base64 format otherwise it will result in an error.
apiVersion: v1
kind: Secret
metadata:
  name: aws-credentials
data:
  accessKey: MTIzNDU2Nzg5MA==
  secretKey: U0VDUkVUMTIzNA==


*** If we want to provide plain value then we can stringData instead of data
apiVersion: v1
kind: Secret
metadata:
  name: aws-credentials
stringData:
  accessKey: 1234567890
  secretKey: SECRET1234

*** This will internally convert above yaml file into file where values are converted into base64 format.


Ingress Controller:
In our K8S Cluster we have one load balancer created out of the box where master node(s) will be load balanced.
when we execute kubectl commands it will go through LB -> master node -> worker node
So This LB is completely for the internal cluster operation like scheduling pods, auto scaling etc.

But our service(s) can be exposed to the external users.
In our K8S cluster webapp service(Angular App) is given to external users so for this we defined a load balancer where all the nodes in the cluster will be attached (because we dont know on which node webapp will run as it might restart and scheduled on another node) and the requests will be given to the webapp.

Suppose if we have another service which needs to exposed to the external users then we have to use another load balancer as the routing path and ports will be different.

With this approach we need 'n' LB for 'n' services that are exposed, and load balancer are costly as it is a hardware component.

We can solve this problem by using Application Load Balancer(ALB) in AWS which will have target group for every service 
1 ALB - n Target Group - n services

But K8S doesn't work well with ALB as it is specific to AWS and K8S want to be cloud agnostic, we can configure K8S to work with ALB but it requires lot of wiring.

*** To solve this problem we can use Ingress Controller behind the load balancer, and we will define the routing rules in the ingress controller
LB - Ingress Controller - n Services

So in any professional K8S cluster we will have 2 Load Balancer, thanks to Ingress Controller.

Ingress Controller On Minikube:
In minikube we can start Ingress Controller just by enabling the addon

minikube addons enable ingress

This command will run nginx ingress-controller pod and a default-http-backend service in kube-system namespace.

By Default Ingress controller has the routing to the default-http-backend service.

Here in minikube we don't have a load balancer so client will directly invoke ingress controller.

Basic Authentication:
use htpasswd for generating Bcrypt value of apache active username and password and save it to auth file.

Generate Secret from that file
kubectl create secret generic mycredentials --from-file auth

Ingress Controller on AWS:
On AWS we need to install Ingress Controller.

wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.30.0/deploy/static/mandatory.yaml
kubectl apply -f mandatory.yaml

We have configure ELB as L4 or L7 this is not applicable in minikube as users will directly invoke ingress controller
wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.30.0/deploy/static/provider/aws/service-l4.yaml
kubectl apply -f service-l4.yaml

wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.30.0/deploy/static/provider/aws/patch-configmap-l4.yaml
kubectl apply -f patch-configmap-l4.yaml

Now we can run routing rule files
kubectl apply -f ingress-public.yaml
kubectl apply -f secure-routing.yaml

Now before testing in the browser we need to configure domains names to a specific ip adress in /etc/hosts file

In Minikube:
{ip-address-of-minikube} fleetman.com
{ip-address-of-minikube} queue.fleetman.com

In AWS:
{ip-address-of-ELB} fleetman.com
{ip-address-of-ELB} queue.fleetman.com


Batch Job:
Difference Between a Pod and a Job
In most of the cases we need to run the pod forever.
But there are situations where we need to run a task and complete within a period, for these requirements we use Job Workload.
Job here in K8S Run to Completion.

Cron Job:
It is just a job which runs based on the schedule field.

Daemon Set:
A DaemonSet ensures that all Nodes run a copy of Pod.
When a Node is added to the cluster then a new pod will be scheduled on that node.
when a Node is deleted from the cluster then the pod will be killed.

Just change 
Kind: Deployment
to
Kind: DaemonSet

and remove the replicas field which is redudant as it will be 1 on each node.

eg: fluent-d which will collect logs in each node.

Stateful Set:
*** StatefulSet is NOT used for Persistence.

**** Services in the K8S exposes an network endpoint to the client so that when a request comes Service will load balance and route to the Pod.

In K8S Pods are treated as Cattle, means Pods will be given a name with a sufix of random characters
Sometimes we need set of pods with known and Predictable names, and you want clients to be able to call them by their name, for this one we need to use StatefulSet.

*** In K8S we call this StatefulSet as PetSet because here pods are Pet not Cattle and the service which exposes network endpoint for the statefulset is called as "Headless Server"


*** In a PetSet or StatefulSet below are the Characterstics:
1. Pods will have Predictable Names i.e., prefix with 0,1,2
2. Pods will always startup in sequence.
3. Clients can call them by name.


*** Databases can't usually be replicated by simple ReplicaSets/Deployments and we need to use there replication process.

*** In a StatefulSet service which exposes network endpoint is called Headless Server, and this server will not route the write or read operation request to one of the pod that is running in the K8S Cluster, instead it will delegates that request to the primary mongo server and for this reason we need to have pods with known names, So that service can call with that name.

*** In statefulset the headless service will expose the network endpoint as follows
{instance-name}.{service-name}
eg: mongo-0.mongodb

mongo url property in spring application file.
spring.data.mongodb.uri=mongodb://mongo-0.mongo.default.svc.cluster.local,mongo-1.mongo.default.svc.cluster.local,mongo-2.mongo.default.svc.cluster.local/fleetman

Here we need to give all instance names because it will iterate through all instances and find the primary instance, this process will happen at the startup or when a node crashes/unreachable.



kubectl get job

kubectl delete job test-job

kubectl describe job test-job

  Type     Reason                Age   From            Message
  ----     ------                ----  ----            -------
  Normal   SuccessfulCreate      70s   job-controller  Created pod: test-job-d7zwm
  Normal   SuccessfulCreate      51s   job-controller  Created pod: test-job-lbg78
  Warning  BackoffLimitExceeded  18s   job-controller  Job has reached the specified backoff limit
[root@ip-172-31-25-62 ~]# 



kubectl delete cronjob cron-job